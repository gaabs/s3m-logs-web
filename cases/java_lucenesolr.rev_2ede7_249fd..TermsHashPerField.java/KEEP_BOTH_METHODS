package org.apache.lucene.index; 

/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException; 
 
import java.util.Comparator; 
import java.util.concurrent.atomic.AtomicLong; 

import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute; 
import org.apache.lucene.document.Fieldable; 
import org.apache.lucene.util.ByteBlockPool; 
import org.apache.lucene.util.BytesRef; 
import org.apache.lucene.util.BytesRefHash; 
import org.apache.lucene.util.BytesRefHash.BytesStartArray; 
import org.apache.lucene.util.BytesRefHash.MaxBytesLengthExceededException; 
 

final  class  TermsHashPerField  extends InvertedDocConsumerPerField {
	
  private static final int HASH_INIT_SIZE = 4;
	

  final TermsHashConsumerPerField consumer;
	

  final TermsHash termsHash;
	

  final TermsHashPerField nextPerField;
	
  
	
  final DocumentsWriterPerThread.DocState docState;
	
  final FieldInvertState fieldState;
	
  TermToBytesRefAttribute termAtt;
	

  // Copied from our perThread
  final IntBlockPool intPool;
	
  final ByteBlockPool bytePool;
	
  final ByteBlockPool termBytePool;
	

  final int streamCount;
	
  final int numPostingInt;
	

  final FieldInfo fieldInfo;
	

  final BytesRefHash bytesHash;
	

  
	
  
	
  
	
  
	
  
	
  
	
 
  ParallelPostingsArray postingsArray;
	
  private final BytesRef termBytesRef;
	
  private final AtomicLong bytesUsed;
	
  
	
  
	

  public TermsHashPerField(DocInverterPerField docInverterPerField, final TermsHashPerThread perThread, final TermsHashPerThread nextPerThread, final FieldInfo fieldInfo) {
    this.perThread = perThread;
    intPool = perThread.intPool;
    bytePool = perThread.bytePool;
    termBytePool = perThread.termBytePool;
    docState = perThread.docState;
    bytesUsed =  perThread.termsHash.trackAllocations?perThread.termsHash.docWriter.bytesUsed:new AtomicLong();

    fieldState = docInverterPerField.fieldState;
    this.consumer = perThread.consumer.addField(this, fieldInfo);
    PostingsBytesStartArray byteStarts = new PostingsBytesStartArray(this, bytesUsed);
    bytesHash = new BytesRefHash(termBytePool, HASH_INIT_SIZE, byteStarts); 
    streamCount = consumer.getStreamCount();
    numPostingInt = 2*streamCount;
    termBytesRef = perThread.termBytesRef;
    this.fieldInfo = fieldInfo;
    if (nextPerThread != null)
      nextPerField = (TermsHashPerField) nextPerThread.addField(docInverterPerField, fieldInfo);
    else
      nextPerField = null;
  }

	

  public TermsHashPerField(DocInverterPerField docInverterPerField, final TermsHash termsHash, final TermsHash nextTermsHash, final FieldInfo fieldInfo) {
    intPool = termsHash.intPool;
    bytePool = termsHash.bytePool;
    termBytePool = termsHash.termBytePool;
    docState = termsHash.docState;
    this.termsHash = termsHash;

    postingsHash = new int[postingsHashSize];
    Arrays.fill(postingsHash, -1);
    bytesUsed(postingsHashSize * RamUsageEstimator.NUM_BYTES_INT);

    fieldState = docInverterPerField.fieldState;
    this.consumer = termsHash.consumer.addField(this, fieldInfo);
    initPostingsArray();

    streamCount = consumer.getStreamCount();
    numPostingInt = 2*streamCount;
    utf8 = termsHash.utf8;
    this.fieldInfo = fieldInfo;
    if (nextTermsHash != null)
      nextPerField = (TermsHashPerField) nextTermsHash.addField(docInverterPerField, fieldInfo);
    else
      nextPerField = null;
  }
	

  
	

  // sugar: just forwards to DW
  private void bytesUsed(long size) {
    if (termsHash.trackAllocations) {
      termsHash.docWriter.bytesUsed(size);
    }
  }

	

  void shrinkHash(int targetSize) {
    // Fully free the bytesHash on each flush but keep the pool untouched
    // bytesHash.clear will clear the ByteStartArray and in turn the ParallelPostingsArray too
    bytesHash.clear(false); 
  }
	

  public void reset() {
    bytesHash.clear(false);
    if (nextPerField != null)
      nextPerField.reset();
  }
	

  @Override
  public void abort() {
    reset();
    if (nextPerField != null)
      nextPerField.abort();
  }
	
  
  
	

  public void initReader(ByteSliceReader reader, int termID, int stream) {
    assert stream < streamCount;
    int intStart = postingsArray.intStarts[termID];
    final int[] ints = intPool.buffers[intStart >> DocumentsWriterRAMAllocator.INT_BLOCK_SHIFT];
    final int upto = intStart & DocumentsWriterRAMAllocator.INT_BLOCK_MASK;
    reader.init(bytePool,
                postingsArray.byteStarts[termID]+stream*ByteBlockPool.FIRST_LEVEL_SIZE,
                ints[upto+stream]);
  }
	

  private void compactPostings() {
    int upto = 0;
    for(int i=0;i<postingsHashSize;i++) {
      if (postingsHash[i] != -1) {
        if (upto < i) {
          postingsHash[upto] = postingsHash[i];
          postingsHash[i] = -1;
        }
        upto++;
      }
    }

    assert upto == numPostings;
    postingsCompacted = true;
  }

	


  /** Collapse the hash table & sort in-place. */
  public int[] sortPostings(Comparator<BytesRef> termComp) {
   return bytesHash.sort(termComp);
  }
	

  
	

  /** Compares term text for two Posting instance and
   *  returns -1 if p1 < p2; 1 if p1 > p2; else 0. */
  int comparePostings(int term1, int term2) {

    if (term1 == term2) {
      // Our quicksort does this, eg during partition
      return 0;
    }

    termBytePool.setBytesRef(termsHash.tr1, postingsArray.textStarts[term1]);
    termBytePool.setBytesRef(termsHash.tr2, postingsArray.textStarts[term2]);

    return termComp.compare(termsHash.tr1, termsHash.tr2);
  }

	

  /** Test whether the text for current RawPostingList p equals
   *  current tokenText in utf8. */
  private boolean postingEquals(final int termID) {
    final int textStart = postingsArray.textStarts[termID];
    final byte[] text = termBytePool.buffers[textStart >> DocumentsWriterRAMAllocator.BYTE_BLOCK_SHIFT];
    assert text != null;

    int pos = textStart & DocumentsWriterRAMAllocator.BYTE_BLOCK_MASK;
    
    final int len;
    if ((text[pos] & 0x80) == 0) {
      // length is 1 byte
      len = text[pos];
      pos += 1;
    } else {
      // length is 2 bytes
      len = (text[pos]&0x7f) + ((text[pos+1]&0xff)<<7);
      pos += 2;
    }

    if (len == utf8.length) {
      final byte[] utf8Bytes = utf8.bytes;
      for(int tokenPos=0;tokenPos<utf8.length;pos++,tokenPos++) {
        if (utf8Bytes[tokenPos] != text[pos]) {
          return false;
        }
      }
      return true;
    } else {
      return false;
    }
  }

	

  private boolean doCall;
	
  private boolean doNextCall;
	

  @Override
  void start(Fieldable f) {
    termAtt = fieldState.attributeSource.getAttribute(TermToBytesRefAttribute.class);
    consumer.start(f);
    if (nextPerField != null) {
      nextPerField.start(f);
    }
  }
	
  
  @Override
  boolean start(Fieldable[] fields, int count) throws IOException {
    doCall = consumer.start(fields, count);
    bytesHash.reinit();
    if (nextPerField != null)
      doNextCall = nextPerField.start(fields, count);
    return doCall || doNextCall;
  }
	

  // Secondary entry point (for 2nd & subsequent TermsHash),
  // because token text has already been "interned" into
  // textStart, so we hash by textStart
  public void add(int textStart) throws IOException {
    int termID = bytesHash.addByPoolOffset(textStart);
    if (termID >= 0) {      // New posting
      // First time we are seeing this token since we last
      // flushed the hash.
      // Init stream slices
      if (numPostingInt + intPool.intUpto > DocumentsWriterRAMAllocator.INT_BLOCK_SIZE)
        intPool.nextBuffer();

<<<<<<< MINE
      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)
=======
      if (DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)
>>>>>>> YOURS
        bytePool.nextBuffer();

      intUptos = intPool.buffer;
      intUptoStart = intPool.intUpto;
      intPool.intUpto += streamCount;

      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;

      for(int i=0;i<streamCount;i++) {
        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);
        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;
      }
      postingsArray.byteStarts[termID] = intUptos[intUptoStart];

      consumer.newTerm(termID);

    } else {
      termID = (-termID)-1;
      int intStart = postingsArray.intStarts[termID];
      intUptos = intPool.buffers[intStart >> DocumentsWriterRAMAllocator.INT_BLOCK_SHIFT];
      intUptoStart = intStart & DocumentsWriterRAMAllocator.INT_BLOCK_MASK;
      consumer.addTerm(termID);
    }
  }
	

  // Primary entry point (for first TermsHash)
  @Override
  void add() throws IOException {

    // We are first in the chain so we must "intern" the
    // term text into textStart address
    // Get the text & hash of this term.
<<<<<<< MINE
    int termID;
    try{
       termID = bytesHash.add(termBytesRef, termAtt.toBytesRef(termBytesRef));
    }catch (MaxBytesLengthExceededException e) {
=======
    int code = termAtt.toBytesRef(utf8);

    int hashPos = code & postingsHashMask;

    // Locate RawPostingList in hash
    int termID = postingsHash[hashPos];

    if (termID != -1 && !postingEquals(termID)) {
      // Conflict: keep searching different locations in
      // the hash table.
      final int inc = ((code>>8)+code)|1;
      do {
        code += inc;
        hashPos = code & postingsHashMask;
        termID = postingsHash[hashPos];
      } while (termID != -1 && !postingEquals(termID));
    }

    if (termID == -1) {

      // First time we are seeing this token since we last
      // flushed the hash.
      final int textLen2 = 2+utf8.length;
      if (textLen2 + bytePool.byteUpto > DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE) {
>>>>>>> YOURS
        // Not enough room in current block
<<<<<<< MINE
=======

        if (utf8.length > DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8) {
>>>>>>> YOURS
          // Just skip this term, to remain as robust as
          // possible during indexing.  A TokenFilter
          // can be inserted into the analyzer chain if
          // other behavior is wanted (pruning the term
          // to a prefix, throwing an exception, etc).
          if (docState.maxTermPrefix == null) {
        final int saved = termBytesRef.length;
            try {
<<<<<<< MINE
          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);
          docState.maxTermPrefix = termBytesRef.toString();
=======
              utf8.length = Math.min(30, DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8);
              docState.maxTermPrefix = utf8.toString();
>>>>>>> YOURS
            } finally {
          termBytesRef.length = saved;
            }
          }
          consumer.skippingLongTerm();
          return;
        }
    if (termID >= 0) {// New posting
      bytesHash.byteStart(termID);
      // Init stream slices
      if (numPostingInt + intPool.intUpto > DocumentsWriterRAMAllocator.INT_BLOCK_SIZE) {
        intPool.nextBuffer();
      }

<<<<<<< MINE
      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {
=======
      if (DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {
>>>>>>> YOURS
        bytePool.nextBuffer();
      }

      intUptos = intPool.buffer;
      intUptoStart = intPool.intUpto;
      intPool.intUpto += streamCount;

      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;

      for(int i=0;i<streamCount;i++) {
        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);
        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;
      }
      postingsArray.byteStarts[termID] = intUptos[intUptoStart];

      consumer.newTerm(termID);

    } else {
      termID = (-termID)-1;
      final int intStart = postingsArray.intStarts[termID];
      intUptos = intPool.buffers[intStart >> DocumentsWriterRAMAllocator.INT_BLOCK_SHIFT];
      intUptoStart = intStart & DocumentsWriterRAMAllocator.INT_BLOCK_MASK;
      consumer.addTerm(termID);
    }

    if (doNextCall)
      nextPerField.add(postingsArray.textStarts[termID]);
  }
	

  int[] intUptos;
	
  int intUptoStart;
	

  void writeByte(int stream, byte b) {
    int upto = intUptos[intUptoStart+stream];
<<<<<<< MINE
    byte[] bytes = bytePool.buffers[upto >> ByteBlockPool.BYTE_BLOCK_SHIFT];
=======
    byte[] bytes = bytePool.buffers[upto >> DocumentsWriterRAMAllocator.BYTE_BLOCK_SHIFT];
>>>>>>> YOURS
    assert bytes != null;
<<<<<<< MINE
    int offset = upto & ByteBlockPool.BYTE_BLOCK_MASK;
=======
    int offset = upto & DocumentsWriterRAMAllocator.BYTE_BLOCK_MASK;
>>>>>>> YOURS
    if (bytes[offset] != 0) {
      // End of slice; allocate a new one
      offset = bytePool.allocSlice(bytes, offset);
      bytes = bytePool.buffer;
      intUptos[intUptoStart+stream] = offset + bytePool.byteOffset;
    }
    bytes[offset] = b;
    (intUptos[intUptoStart+stream])++;
  }
	

  public void writeBytes(int stream, byte[] b, int offset, int len) {
    // TODO: optimize
    final int end = offset + len;
    for(int i=offset;i<end;i++)
      writeByte(stream, b[i]);
  }
	

  void writeVInt(int stream, int i) {
    assert stream < streamCount;
    while ((i & ~0x7F) != 0) {
      writeByte(stream, (byte)((i & 0x7f) | 0x80));
      i >>>= 7;
    }
    writeByte(stream, (byte) i);
  }
	

  @Override
  void finish() throws IOException {
    consumer.finish();
    if (nextPerField != null)
      nextPerField.finish();
  }
	
  
  private static final  class  PostingsBytesStartArray  extends BytesStartArray {
		

    private final TermsHashPerField perField;
		
    private final AtomicLong bytesUsed;
		

    private PostingsBytesStartArray(
        TermsHashPerField perField, AtomicLong bytesUsed) {
      this.perField = perField;
      this.bytesUsed = bytesUsed;
    }
		
    
    @Override
    public int[] init() {
      if(perField.postingsArray == null) { 
        perField.postingsArray = perField.consumer.createPostingsArray(2);
        bytesUsed.addAndGet(perField.postingsArray.size * perField.postingsArray.bytesPerPosting());
      }
      return perField.postingsArray.textStarts;
    }
		

    @Override
    public int[] grow() {
      ParallelPostingsArray postingsArray = perField.postingsArray;
      final int oldSize = perField.postingsArray.size;
      postingsArray = perField.postingsArray = postingsArray.grow();
      bytesUsed
          .addAndGet((postingsArray.bytesPerPosting() * (postingsArray.size - oldSize)));
      return postingsArray.textStarts;
    }
		

    @Override
    public int[] clear() {
      if(perField.postingsArray != null) {
        bytesUsed.addAndGet(-perField.postingsArray.size * perField.postingsArray.bytesPerPosting());
        perField.postingsArray = null;
      }
      return null;
    }
		

    @Override
    public AtomicLong bytesUsed() {
      return bytesUsed;
    }

	}
	

  /** Called when postings hash is too small (> 50%
   *  occupied) or too large (< 20% occupied). */
  void rehashPostings(final int newSize) {

    final int newMask = newSize-1;

    int[] newHash = new int[newSize];
    Arrays.fill(newHash, -1);
    for(int i=0;i<postingsHashSize;i++) {
      int termID = postingsHash[i];
      if (termID != -1) {
        int code;
        if (termsHash.primary) {
          final int textStart = postingsArray.textStarts[termID];
          final int start = textStart & DocumentsWriterRAMAllocator.BYTE_BLOCK_MASK;
          final byte[] text = bytePool.buffers[textStart >> DocumentsWriterRAMAllocator.BYTE_BLOCK_SHIFT];
          code = 0;

          final int len;
          int pos;
          if ((text[start] & 0x80) == 0) {
            // length is 1 byte
            len = text[start];
            pos = start+1;
          } else {
            len = (text[start]&0x7f) + ((text[start+1]&0xff)<<7);
            pos = start+2;
          }

          final int endPos = pos+len;
          while(pos < endPos) {
            code = (code*31) + text[pos++];
          }
        } else {
          code = postingsArray.textStarts[termID];
        }

        int hashPos = code & newMask;
        assert hashPos >= 0;
        if (newHash[hashPos] != -1) {
          final int inc = ((code>>8)+code)|1;
          do {
            code += inc;
            hashPos = code & newMask;
          } while (newHash[hashPos] != -1);
        }
        newHash[hashPos] = termID;
      }
    }

    postingsHashMask = newMask;
    postingsHash = newHash;

    postingsHashSize = newSize;
    postingsHashHalfSize = newSize >> 1;
  }


}
